{
    "game": "Walker2d-v2",
    "mode": "ppo",
    "value_calc": "gae",
    "num_minibatches": 32,
    "policy_net_type": "CtsPolicy",
    "value_net_type": "ValueNet",
    "num_actors": 1,
    "norm_states": true,
    "norm_rewards": "returns",
    "ppo_lr": -1,
    "ppo_lr_adam": 3e-4,
    "val_lr":2e-5,
    "t": 2000,
    "entropy_coeff": 0.0,
    "clip_eps": 0.2,
    "gamma": 0.99,
    "lambda": 0.95,
    "max_kl": 0.01,
    "max_kl_final": 0.01,
    "fisher_frac_samples": 0.1,
    "cg_steps": 10,
    "damping": 0.1,
    "max_backtrack": 10,
    "val_epochs": 10,
    "anneal_lr": true,
    "ppo_epochs": 10,
    "clip_rewards": 10.0,
    "clip_observations": 10.0,
    "clip_advantages": 1e8,
    "sign_advantages": false,
    "norm_advantages": true,
    "train_steps": 500,
    "initialization": "orthogonal",
    "share_weights": false,
    "value_multiplier": 0.1,
    "value_clipping": true,
    "cpu": true,
    "advanced_logging": false,
    "log_every": 10,
    "adam_eps": 1e-5,
    "kl_approximation_iters": -1,
    "save_iters": 0,
    "out_dir": null,
    "clip_grad_norm": -1,
    "anneal_kl_penalty_coeff": false,
    "kl_closed_form": true,
    "kl_npg_form": false,
    "reward_gaussian_noise": 0.0,
    "reward_uniform_noise": 0.0,
    "reward_sparsity": 0.0
}
