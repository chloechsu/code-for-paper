Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/4599676c-8bf3-43c7-b791-a205a898544c
ERROR 'clip_advantages'
Process Process-22:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/1442fd1f-d536-4bbd-80c4-e934cd6132ca
ERROR 'clip_advantages'
Process Process-17:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/30b832f4-5357-42a5-bab4-bba9f4f0e8a6
ERROR 'clip_advantages'
Process Process-6:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/14d7a900-4cac-4dd3-b410-e31487936db2
ERROR 'clip_advantages'
Process Process-11:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/a15ab87c-3f10-49fd-ac39-d05c7ad6af6a
ERROR 'clip_advantages'
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/e80e2fe1-2f43-459f-9bb2-a6601f17a8f5
ERROR 'clip_advantages'
Process Process-19:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/9306e193-3f3b-43b0-8422-17aab0572b3c
ERROR 'clip_advantages'
Process Process-4:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/29980b03-98f9-4f1c-9b20-087edf96b621
ERROR 'clip_advantages'
Process Process-5:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/0c7838b5-5cc6-4366-9e93-d5d081118099
ERROR 'clip_advantages'
Process Process-24:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/c763e5b4-eef7-49c1-a52d-fa82c8a959f6
ERROR 'clip_advantages'
Process Process-3:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/8858c7e8-9e73-4adb-a9e4-63d972fbf550
ERROR 'clip_advantages'
Process Process-2:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/d5221f45-69a2-413d-bec9-5c19b9d24f36
ERROR 'clip_advantages'
Process Process-7:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/deb18520-e01b-4aa6-b926-4614855e786f
ERROR 'clip_advantages'
Process Process-10:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/99e74127-25dc-4ace-bf58-53cac3d198ae
ERROR 'clip_advantages'
Process Process-18:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/d179bb19-7f0f-4e66-ac1b-4f60e0316e1f
ERROR 'clip_advantages'
Process Process-8:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/03be4e7d-2de3-4a7b-9414-d4c38b0268f5
ERROR 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/4625a3ab-151a-4d47-833b-7a6cd116e2c5
ERROR 'clip_advantages'
Process Process-16:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Process Process-15:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/192b43ca-3336-4e95-a8ee-beb6295c7eca
ERROR 'clip_advantages'
Process Process-14:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/9a132fa0-2392-446e-84c2-4642a854b9d1
ERROR 'clip_advantages'
Process Process-20:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/d0c585a4-3957-4ebf-9bd1-6eb4ddd144db
ERROR 'clip_advantages'
Process Process-9:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/81d5e688-97c4-4147-87e6-d765b3fdf8fc
ERROR 'clip_advantages'
Process Process-13:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/0d776618-0c5b-4499-b312-269dbb7e0345
ERROR 'clip_advantages'
Process Process-12:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/83d1062d-2dc2-406d-8302-5d3d19ad99d6
ERROR 'clip_advantages'
Process Process-21:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
Logging in: /home/eecs/chloehsu/ppo_ablation/code-for-paper/src/reward_scaling/agents/f5c13104-346e-4ef2-a815-33ccdd9bdc29
ERROR 'clip_advantages'
Process Process-23:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "run_agents.py", line 20, in run_single_config
    raise e
  File "run_agents.py", line 17, in run_single_config
    main(params)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/run.py", line 119, in main
    mean_reward = p.train_step()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 497, in train_step
    surr_loss, val_loss = self.take_steps(saps)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/agent.py", line 454, in take_steps
    surr_loss = self.policy_step(*args).mean()
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/steps.py", line 261, in ppo_step
    old=batch_old_log_ps, clip_adv=params.CLIP_ADVANTAGES)
  File "/home/eecs/chloehsu/ppo_ablation/code-for-paper/src/policy_gradients/torch_utils.py", line 35, in __getattr__
    return self.params[x.lower()]
KeyError: 'clip_advantages'
